---
title: "Report2"
author: "Sander Miesen, Gaelle Verdon, Emma Boehly, Constance de Trogoff"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r import, include=FALSE}
 load("projectData.RData")
```

# Introduction

Investigating the causes of crime is a complicated task. Indeed, many external factors, such as civil wars and worker strikes could influence any region’s crime rates drastically, and these factors are hard to control for. Our dataset, collected by John Clay between 1849 and 1853, has the particularity of having very few of those disturbances as there were, in his own words, “no political or social excitement [and] no cessation of the employment”. Additionally, in 1851 there was a Census, thus making the recording of the number of inhabitants as accurate as possible. These two factors make this an ideal case study, controlling for many external variables.

In this report, we aim to investigate how counties’ number of criminals per 100’000 inhabitants are affected by the number of ale and beer houses per 100’000 inhabitants, as well as the number of people attending at school per 10’000 inhabitants and the number of people attending at public worship per 2’000 inhabitants. These variables are interesting as they relate to three more general socio-cultural themes which often come up when talking about crime, namely drunkenness, education and religion. Through the running of regression models, we hope to find significant relationships between the variables and the crime rate, which could eventually help set up measures to decrease it.


# EDA

```{r str, echo=FALSE}
 str(data)
```

(We could observe that Criminals per 100k are considered as character although they are discrete data. Thus we converted them to integer.
Furthermore, region_name and region_code were identified as characters although they are factors. Thus we converted them to factors.)

```{r convert, include=FALSE}
data$criminals_per_100k <- as.integer(data$criminals_per_100k)
data$region_code <- as.factor(data$region_code)
data$region_name <- as.factor(data$region_name)
str(data)
attach(data)
```

To start our data exploration, we first performed a univariate data analysis of the predictors and of the response variable of interest meaning the number of criminals per 100k per county. We have 3 numerical features : the number of Ale beer houses per 100k per county, the of public school attendants per 100k per county, and the number of public worship attendants per 100k per county, and 3 categorical features : the county names, the region names and the region codes. By observing the data, we could notice that each sample corresponds to a different county name and thus we ignored this last feature for the following of our analysis. We could also remark that each region name corresponds to a region code, except for "South Midland" which had the same region code as "South Eastern". We thus replaced the region_code "1" of South Eastern to factor "0". Proportions of categorical features are indicated in Table 1. (Faire un tableau latex)

```{r heatmap, include=FALSE}

cont_table = table(data$region_name, data$region_code)
# Convert the table to a matrix
cont_matrix <- as.matrix(cont_table)

colors <- colorRampPalette(c("lightyellow", "blue"))(100)

# Create the heatmap
heatmap(cont_matrix, 
        col = colors, 
        scale = "none",  # To prevent scaling of values
        Rowv = NA,       # To suppress row dendrogram
        Colv = NA,       # To suppress column dendrogram
        margins = c(5, 5))  # Set margins

# Add a color key
legend("bottomleft", legend = c("0", "Above 0"), fill = c("lightyellow", "blue"), bty = "n")

```

```{r summary2, echo=FALSE}
data$region_code <- factor(data$region_code, levels = c(levels(data$region_code), "0"))
data[data$region_name == "South Eastern", "region_code"] <- factor("0")
attach(data)

categorical_features <- sapply(data, function(x) is.factor(x) | is.character(x))
categorical_data <- data[, categorical_features]

summary(categorical_data)
```

We first computed informations about the center of each numerical predictors distribution (the mean and the median), and about the spread (with the minimum and maximum value, the lower percentile 25 and the upper percentile 75 as well as the standard deviations). Results are displayed in Table 2.

Faire un tableau latex avec les données suivantes en rajoutant Standard deviation (SD), Interquartile range (IQR), Median Absolute Deviation (MAD)
```{r summary, echo=FALSE}
# Assuming 'data' is your dataset

# Extract numerical features and corresponding data
numerical_features <- sapply(data, is.numeric)
numerical_data <- data[, numerical_features]

summary(numerical_data)
```

While the mean is very sensitive to outliers, the median is not. Thus by comparing the two, we can have insights on the presence of outliers. We can observe that the mean and the median are quite similar relative to the spread for all three numerical features, infering against the presence of outliers. In order to have a visual summary of the distributions, boxplot of the predictors are shown in Figure 1. They appear to be quite symmetric, while being slightly left-skewed for the Beer Houses, and the Worship attendants and slightly right-skewed for the Public School attendants. 

```{r boxplots2, echo=FALSE}
library(packHV)
layout(matrix(c(1, 1, 2), nrow = 1), widths = c(1.25, 1))

# Creating a single boxplot with different colors for each feature
boxplot(ale_beer_houses_per_100k, 
        attendants_public_school_per_10k, 
        attendants_public_worship_per_2k, 
        main = "Comparison of Various Features per County",
        names = c("Beer Houses", "School Attendants", "Worship Attendants"),
        ylab = "Count per 100k")
hist_boxplot(criminals_per_100k, breaks=20, ymax=6)
```

Next, to better visualize the response variable of interest, i.e. the number of criminals per county per 100k, we plotted its distribution with a histogran and a boxplot in Figure 2 which is slightly left skewed. For a first naive comparison of its distribution across regions, we displayed seperate boxplots for each region in Figure 3. We could observe clear differences between the number of criminals' median and us ordered them in ascending order. As we can hypothesize a statistical difference between region 8 (corresponding to "Northern") which as a median number of criminals lower than 100, and region 4 (corresponding to West Midland) which has a median number of criminal higher than 200, we compared the predictor's distribution of those 2 regions in Figure 4.


```{r bivariate boxplot, echo=FALSE}
factor_colors <- c("red", "cyan", "magenta", "orange", "purple", "yellow", "pink", "green", "blue")

# Order region codes by median value
ordered_region <- names(sort(tapply(criminals_per_100k, region_code, median)))

# Create boxplots ordered by median
boxplot(criminals_per_100k ~ factor(region_code, levels = ordered_region), 
        main = "Criminals per 100k by Region Code",
        col = factor_colors,
        xlab = "Region Code",
        ylab = "Criminals per 100k")

```
```{r dist,echo=FALSE}
factor_colors <- c("blue", "red")
subset_data <- data[data$region_code %in% c(factor("4"), factor("8")), ]
subset_data$region_code <- factor(subset_data$region_code, levels = c(factor("4"), factor("8")))

par(mfrow=c(1,3))
boxplot(subset_data$ale_beer_houses_per_100k ~ subset_data$region_code, main = "", col = factor_colors, xlab="Region code")
boxplot(subset_data$attendants_public_school_per_10k ~ subset_data$region_code, main = "", col = factor_colors, xlab = "Region code")
boxplot(subset_data$attendants_public_worship_per_2k ~ subset_data$region_code, main = "", col = factor_colors, xlab ="Region code")
```


```{r correlation3, echo=FALSE, warning=FALSE}
factor_colors <- c("pink", "purple", "yellow", "orange", "blue", "magenta", "green", "cyan", "red")
# Customize upper panel
upper.panel<-function(x, y){
  points(x,y, pch=19, col=factor_colors[data$region_code])
  r <- round(cor(x, y), digits=2)
  txt <- paste0("R = ", r)
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  text(0.5, 0.9, txt)
}
pairs(data[,5:7], lower.panel = NULL, 
      upper.panel = upper.panel)

```
```{r test, include=FALSE}

plot(criminals_per_100k~attendants_public_worship_per_2k)
plot(criminals_per_100k~ale_beer_houses_per_100k)
print(factor_colors[data$region_code])
print(data)
```

# Model Fitting
In this project, we aim at finding a linear regression model predicting the number of criminals per 100 000 people. As stated in the project description, we will only consider numerical variable in our model, that is to say, only Ale/Beer Houses per 100 000 people, Attendants at public school per 10 000 people, and Attendants at public worship per 2000 people will be taken into account in our model. 
In the following, we will first look at the model prediction based on these 3 variables and then we will try to find the best model possible by following a backward model selection process. This process will be based on adjusted R squared method ($R_{adj}^{2}$ in the following) and the Akaike’s Information Criterion (AIC in the following). We will try to lowering the AIC while increasing R squared adjusted in our models. 


|Variable Name     | Description                             |
|----------------- | ----------------------------------------|
|Y                 |    Number of criminals per 100k         |
|$\beta_{0}$       |    Ale/Beer Houses per 100k             |
|$\beta_{1}$       |    Attendants at public school per 10k  |   
|$\beta_{2}$       |    Attendants at public worship per 2k  |
*Table XXXXXXXXXXXXXXXXXXXXXXX : Summary of variable names *

**Model 1: ** \
$Y = 172.88 + 0.12\beta_{0} + 0.10\beta_{1} + 0.039\beta_{2}$ \ 
This model gives a $R_{adj}^{2}$ of 0.2619 and an AIC of 405.0494 . 
Now that we have out reference model, let's try to optimize it. In the EDA paragraph, we saw that attendants at public school per 10k and attendants at public worship per 2k have a positive correlation of 0.6, therefore, we decided to remove the latest in the next model. 


**Model 2: ** \
$Y = 178.81 + 0.13\beta_{0} -0.077\beta_{1}$  \
This model gives a $R_{adj}^{2}$ of 0.26 and an AIC of 404.041 . We can see that the model has a highest $R_{adj}^{2}$ and lower AIC, which means that the model is better than model 1.
In the next model, we then wanted to see whether attendants at public worship per 2k contributes more to the performance of the model compared to attendants at public school per 10k.


**Model 3: ** \
$Y = 121.26 + \beta_{0} + 0.12\beta_{0} - 0.017\beta{2}$ \ 
This model gives a $R_{adj}^{2}$ of 0.177 and an AIC of 408.52  which corresponds to poorest results compared to model 2.
To make sure that attendants at public school per 10k really contributes to the improvement of the model, we decided to remove it in the next model. Attendants at public worship per 2k contributes is only removed in the next model model since we already demonstrated that it wasn't contributing to the increase in performance. 
 
 
**Model 4: ** \
$Y = 109.34 + 0.12\beta_{0}$ \
This model gives a $R_{adj}^{2}$ of 0.19 and an AIC of 406.75 . Since both the $R_{adj}^{2}$ and the AIC are respectively lower and higher than for model 2, it means that attendants at public school per 10k does contribute to the increase of performance of our model. 
Finally, we wanted to make sure that attendants at public school per 10k is not the only contributor to the performance of our model. To do that, we will only consider this variable in the last model.


**Model 5:**\
$Y = 209.42 - 0.059\beta_{1}$ \
This model gives a $R_{adj}^{2}$ of 0.15 and an AIC of 414.22 . Since both the $R_{adj}^{2}$ and the AIC are respectively lower and higher than for model 2, it means that ale_beer_houses_per_100k also contributes to the performance of our model. 

|   Model   | $R_{adj}^{2}$ | AIC       |
| --------- | ------------- | --------- |
| Model 1   |  0.2619       | 405.0494  |
| Model 2   |  0.2638       | 404.0409  |
| Model 3   |  0.1767       | 408.5154  |
| Model 4   |  0.1936       | 406.7524  |
| Model 5   |  0.1533       | 414.2217  |

*Table XXXXXXXXXXXXXXXXXXXXXXX : Summary of the $R_{adj}^{2}$ and AIC for each model considered *

# Model selection
In the last section, we computed different models to determine which one is better. We saw that model 1 and model 2 gave us very close results when looking at R squared adjusted, however, looking at AIC values, we can see that model 2 has a lower one and we therefore decided to consider model 2 as our best model.
The final model selected is $Y = 178.81 + 0.13\beta_{0} -0.077\beta_{1}$ (model2)

# Discussion

# Conclusion